apiVersion: tekton.dev/v1
kind: PipelineRun
metadata:
  annotations:
    pipelines.kubeflow.org/run_name: KFP Tekton Run
    tekton.dev/artifact_items: >-
      {"get-data": [["output",
      "$(workspaces.get-data.path)/artifacts/train-upload-stock-kfp-1c509/$(context.taskRun.name)/output"]],
      "train-model": [["output",
      "$(workspaces.train-model.path)/artifacts/train-upload-stock-kfp-1c509/$(context.taskRun.name)/output"]],
      "upload": []}
    tekton.dev/artifact_bucket: pipeline-artifacts
    tekton.dev/template: ''
    sidecar.istio.io/inject: 'false'
    tekton.dev/artifact_endpoint_scheme: 'http://'
    tekton.dev/output_artifacts: >-
      {"get-data": [{"key": "artifacts/$PIPELINERUN/get-data/output.tgz",
      "name": "get-data-output", "path": "/tmp/outputs/output/data"}],
      "train-model": [{"key": "artifacts/$PIPELINERUN/train-model/output.tgz",
      "name": "train-model-output", "path": "/tmp/outputs/output/data"}]}
    tekton.dev/artifact_endpoint: >-
      https://minio-s3-fraud-detection.apps.aisrhods-wx.8goc.p1.openshiftapps.com
    pipelines.kubeflow.org/pipeline_spec: '{"name": "train_upload_stock_kfp"}'
    pipelines.kubeflow.org/big_data_passing_format: >-
      $(workspaces.$TASK_NAME.path)/artifacts/train-upload-stock-kfp-1c509/$TASKRUN_NAME/$TASK_PARAM_NAME
    tekton.dev/input_artifacts: >-
      {"train-model": [{"name": "get-data-output", "parent_task": "get-data"}],
      "upload": [{"name": "train-model-output", "parent_task": "train-model"}]}
  resourceVersion: '484211495'
  name: train-upload-stock-kfp-1c509
  uid: 34d2d2c1-47df-4b8b-b2d0-f4bcf287bbd6
  creationTimestamp: '2024-02-23T14:08:23Z'
  generation: 1
  managedFields:
    - apiVersion: tekton.dev/v1beta1
      fieldsType: FieldsV1
      fieldsV1:
        'f:metadata':
          'f:annotations':
            'f:tekton.dev/artifact_endpoint': {}
            'f:pipelines.kubeflow.org/pipeline_spec': {}
            'f:pipelines.kubeflow.org/run_name': {}
            .: {}
            'f:pipelines.kubeflow.org/big_data_passing_format': {}
            'f:tekton.dev/artifact_items': {}
            'f:tekton.dev/input_artifacts': {}
            'f:tekton.dev/template': {}
            'f:sidecar.istio.io/inject': {}
            'f:tekton.dev/artifact_bucket': {}
            'f:tekton.dev/artifact_endpoint_scheme': {}
            'f:tekton.dev/output_artifacts': {}
          'f:labels':
            .: {}
            'f:custom.tekton.dev/originalPipelineRun': {}
            'f:pipeline/runid': {}
            'f:pipelines.kubeflow.org/generation': {}
            'f:pipelines.kubeflow.org/pipelinename': {}
        'f:spec':
          .: {}
          'f:pipelineSpec':
            .: {}
            'f:tasks': {}
            'f:workspaces': {}
          'f:serviceAccountName': {}
          'f:workspaces': {}
      manager: apiserver
      operation: Update
      time: '2024-02-23T14:08:23Z'
    - apiVersion: tekton.dev/v1beta1
      fieldsType: FieldsV1
      fieldsV1:
        'f:metadata':
          'f:finalizers':
            .: {}
            'v:"chains.tekton.dev/pipelinerun"': {}
      manager: openshift-pipelines-chains-controller
      operation: Update
      time: '2024-02-23T14:08:23Z'
    - apiVersion: tekton.dev/v1
      fieldsType: FieldsV1
      fieldsV1:
        'f:metadata':
          'f:labels':
            'f:tekton.dev/pipeline': {}
      manager: openshift-pipelines-controller
      operation: Update
      time: '2024-02-23T14:08:24Z'
    - apiVersion: tekton.dev/v1
      fieldsType: FieldsV1
      fieldsV1:
        'f:status':
          'f:childReferences': {}
          'f:conditions': {}
          'f:pipelineSpec':
            .: {}
            'f:tasks': {}
            'f:workspaces': {}
          'f:startTime': {}
      manager: openshift-pipelines-controller
      operation: Update
      subresource: status
      time: '2024-02-23T14:08:50Z'
  namespace: fraud-detection
  finalizers:
    - chains.tekton.dev/pipelinerun
  labels:
    custom.tekton.dev/originalPipelineRun: train-upload-stock-kfp-1c509
    pipeline/runid: 1c509116-54de-4946-b6aa-ed1edd454220
    pipelines.kubeflow.org/generation: ''
    pipelines.kubeflow.org/pipelinename: ''
    tekton.dev/pipeline: train-upload-stock-kfp-1c509
spec:
  pipelineSpec:
    tasks:
      - name: get-data
        taskSpec:
          metadata:
            annotations:
              pipelines.kubeflow.org/component_spec_digest: >-
                {"name": "Get data", "outputs": [{"name": "output"}], "version":
                "Get
                data@sha256=a840b31ffc61f2af6a40651f5ee87a07d2c5945073e00ebc9470ccdd8c5bfdfd"}
            labels:
              pipelines.kubeflow.org/cache_enabled: 'true'
          results:
            - description: /tmp/outputs/output/data
              name: output
              type: string
            - name: taskrun-name
              type: string
          spec: null
          steps:
            - args:
                - '--output'
                - >-
                  $(workspaces.get-data.path)/artifacts/train-upload-stock-kfp-1c509/$(context.taskRun.name)/output
              command:
                - sh
                - '-ec'
                - |
                  program_path=$(mktemp)
                  printf "%s" "$0" > "$program_path"
                  python3 -u "$program_path" "$@"
                - >
                  def _make_parent_dirs_and_return_path(file_path: str):
                      import os
                      os.makedirs(os.path.dirname(file_path), exist_ok=True)
                      return file_path

                  def get_data(output_path):
                      import urllib.request
                      print("starting download...")
                      url = "https://github.com/cfchase/fraud-detection-notebooks/raw/main/data/card_transdata.csv"
                      urllib.request.urlretrieve(url, output_path)
                      print("done")

                  import argparse

                  _parser = argparse.ArgumentParser(prog='Get data',
                  description='')

                  _parser.add_argument("--output", dest="output_path",
                  type=_make_parent_dirs_and_return_path, required=True,
                  default=argparse.SUPPRESS)

                  _parsed_args = vars(_parser.parse_args())


                  _outputs = get_data(**_parsed_args)
              computeResources: {}
              env:
                - name: ORIG_PR_NAME
                  valueFrom:
                    fieldRef:
                      fieldPath: 'metadata.labels[''custom.tekton.dev/originalPipelineRun'']'
              image: >-
                quay.io/modh/runtime-images:runtime-cuda-tensorflow-ubi9-python-3.9-2023a-20230817-b7e647e
              name: main
            - command:
                - sh
                - '-ec'
                - >-
                  echo -n "$(context.taskRun.name)" >
                  "$(results.taskrun-name.path)"
              computeResources: {}
              image: busybox
              name: output-taskrun-name
            - command:
                - sh
                - '-ec'
                - >
                  set -exo pipefail

                  TOTAL_SIZE=0

                  copy_artifact() {

                  if [ -d "$1" ]; then
                    tar -czvf "$1".tar.gz "$1"
                    SUFFIX=".tar.gz"
                  fi

                  ARTIFACT_SIZE=`wc -c "$1"${SUFFIX} | awk '{print $1}'`

                  TOTAL_SIZE=$( expr $TOTAL_SIZE + $ARTIFACT_SIZE)

                  touch "$2"

                  if [[ $TOTAL_SIZE -lt 3072 ]]; then
                    if [ -d "$1" ]; then
                      tar -tzf "$1".tar.gz > "$2"
                    elif ! awk "/[^[:print:]]/{f=1} END{exit !f}" "$1"; then
                      cp "$1" "$2"
                    fi
                  fi

                  }

                  copy_artifact
                  $(workspaces.get-data.path)/artifacts/train-upload-stock-kfp-1c509/$(context.taskRun.name)/output
                  $(results.output.path)
              computeResources: {}
              env:
                - name: ORIG_PR_NAME
                  valueFrom:
                    fieldRef:
                      fieldPath: 'metadata.labels[''custom.tekton.dev/originalPipelineRun'']'
              image: busybox
              name: copy-results-artifacts
              onError: continue
            - args:
                - >
                  if [ -d /tekton/results ]; then mkdir -p
                  /tekton/home/tep-results; mv /tekton/results/*
                  /tekton/home/tep-results/ || true; fi
              command:
                - sh
                - '-c'
              computeResources: {}
              image: >-
                registry.access.redhat.com/ubi8/ubi-micro@sha256:396baed3d689157d96aa7d8988fdfea7eb36684c8335eb391cf1952573e689c1
              name: move-all-results-to-tekton-home
            - args:
                - >
                  #!/usr/bin/env sh

                  push_artifact() {
                      workspace_dir=$(echo $(context.taskRun.name) | sed -e "s/$(context.pipeline.name)-//g")
                      workspace_dest=/workspace/${workspace_dir}/artifacts/$(context.pipelineRun.name)/$(context.taskRun.name)
                      artifact_name=$(basename $2)

                      aws_cp() {

                        aws s3 --endpoint https://minio-s3-fraud-detection.apps.aisrhods-wx.8goc.p1.openshiftapps.com cp $1.tgz s3://pipeline-artifacts/artifacts/$PIPELINERUN/$PIPELINETASK/$1.tgz

                      }

                      if [ -f "$workspace_dest/$artifact_name" ]; then
                          echo sending to: ${workspace_dest}/${artifact_name}
                          tar -cvzf $1.tgz -C ${workspace_dest} ${artifact_name}
                          aws_cp $1
                      elif [ -f "$2" ]; then
                          tar -cvzf $1.tgz -C $(dirname $2) ${artifact_name}
                          aws_cp $1
                      else
                          echo "$2 file does not exist. Skip artifact tracking for $1"
                      fi
                  }

                  push_log() {
                      cat /var/log/containers/$PODNAME*$NAMESPACE*step-main*.log > step-main.log
                      push_artifact main-log step-main.log
                  }

                  strip_eof() {
                      if [ -f "$2" ]; then
                          awk 'NF' $2 | head -c -1 > $1_temp_save && cp $1_temp_save $2
                      fi
                  }

                  push_artifact "output" "/tekton/home/tep-results/"$(basename
                  "$(workspaces.get-data.path)/artifacts/train-upload-stock-kfp-1c509/$(context.taskRun.name)/output")
              command:
                - sh
                - '-c'
              computeResources: {}
              env:
                - name: ARTIFACT_BUCKET
                  valueFrom:
                    fieldRef:
                      fieldPath: 'metadata.annotations[''tekton.dev/artifact_bucket'']'
                - name: ARTIFACT_ENDPOINT
                  valueFrom:
                    fieldRef:
                      fieldPath: 'metadata.annotations[''tekton.dev/artifact_endpoint'']'
                - name: ARTIFACT_ENDPOINT_SCHEME
                  valueFrom:
                    fieldRef:
                      fieldPath: >-
                        metadata.annotations['tekton.dev/artifact_endpoint_scheme']
                - name: ARTIFACT_ITEMS
                  valueFrom:
                    fieldRef:
                      fieldPath: 'metadata.annotations[''tekton.dev/artifact_items'']'
                - name: PIPELINETASK
                  valueFrom:
                    fieldRef:
                      fieldPath: 'metadata.labels[''tekton.dev/pipelineTask'']'
                - name: PIPELINERUN
                  valueFrom:
                    fieldRef:
                      fieldPath: 'metadata.labels[''tekton.dev/pipelineRun'']'
                - name: PODNAME
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.name
                - name: NAMESPACE
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.namespace
                - name: AWS_ACCESS_KEY_ID
                  valueFrom:
                    secretKeyRef:
                      key: accesskey
                      name: mlpipeline-minio-artifact
                - name: AWS_SECRET_ACCESS_KEY
                  valueFrom:
                    secretKeyRef:
                      key: secretkey
                      name: mlpipeline-minio-artifact
                - name: ARCHIVE_LOGS
                  value: 'false'
                - name: TRACK_ARTIFACTS
                  value: 'true'
                - name: STRIP_EOF
                  value: 'true'
              image: >-
                registry.redhat.io/rhoai/odh-ml-pipelines-artifact-manager-rhel8@sha256:83248106ac34fb8b9a17fc372f7b6172ef224b79a2627add9e97c7551b2da444
              name: copy-artifacts
          workspaces:
            - name: get-data
        workspaces:
          - name: get-data
            workspace: train-upload-stock-kfp
      - name: train-model
        params:
          - name: get-data-trname
            value: $(tasks.get-data.results.taskrun-name)
        runAfter:
          - get-data
        taskSpec:
          metadata:
            annotations:
              pipelines.kubeflow.org/component_spec_digest: >-
                {"name": "Train model", "outputs": [{"name": "output"}],
                "version": "Train
                model@sha256=eaddeda6a61a76fc58aef31f1b7e762a9f8354045a1cde36986495e6568341ba"}
            labels:
              pipelines.kubeflow.org/cache_enabled: 'true'
          params:
            - name: get-data-trname
              type: string
          results:
            - description: /tmp/outputs/output/data
              name: output
              type: string
            - name: taskrun-name
              type: string
          spec: null
          steps:
            - args:
                - '--input'
                - >-
                  $(workspaces.train-model.path)/artifacts/train-upload-stock-kfp-1c509/$(params.get-data-trname)/output
                - '--output'
                - >-
                  $(workspaces.train-model.path)/artifacts/train-upload-stock-kfp-1c509/$(context.taskRun.name)/output
              command:
                - sh
                - '-c'
                - >-
                  (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
                  --quiet --no-warn-script-location 'tf2onnx' 'seaborn' ||
                  PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
                  --no-warn-script-location 'tf2onnx' 'seaborn' --user) && "$0"
                  "$@"
                - sh
                - '-ec'
                - |
                  program_path=$(mktemp)
                  printf "%s" "$0" > "$program_path"
                  python3 -u "$program_path" "$@"
                - >
                  def _make_parent_dirs_and_return_path(file_path: str):
                      import os
                      os.makedirs(os.path.dirname(file_path), exist_ok=True)
                      return file_path

                  def train_model(input_path, output_path):
                      import numpy as np
                      import pandas as pd
                      from keras.models import Sequential
                      from keras.layers import Dense, Dropout, BatchNormalization, Activation
                      from sklearn.model_selection import train_test_split
                      from sklearn.preprocessing import StandardScaler
                      from sklearn.utils import class_weight
                      import tf2onnx
                      import onnx
                      import pickle
                      from pathlib import Path

                      # Load the CSV data which we will use to train the model.
                      # It contains the following fields:
                      #   distancefromhome - The distance from home where the transaction happened.
                      #   distancefromlast_transaction - The distance from last transaction happened.
                      #   ratiotomedianpurchaseprice - Ratio of purchased price compared to median purchase price.
                      #   repeat_retailer - If it's from a retailer that already has been purchased from before.
                      #   used_chip - If the (credit card) chip was used.
                      #   usedpinnumber - If the PIN number was used.
                      #   online_order - If it was an online order.
                      #   fraud - If the transaction is fraudulent.
                      Data = pd.read_csv(input_path)

                      # Set the input (X) and output (Y) data.
                      # The only output data we have is if it's fraudulent or not, and all other fields go as inputs to the model.

                      X = Data.drop(columns = ['repeat_retailer','distance_from_home', 'fraud'])
                      y = Data['fraud']

                      # Split the data into training and testing sets so we have something to test the trained model with.

                      # X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, stratify = y)
                      X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, shuffle = False)

                      X_train, X_val, y_train, y_val = train_test_split(X_train,y_train, test_size = 0.2, stratify = y_train)

                      # Scale the data to remove mean and have unit variance. This means that the data will be between -1 and 1, which makes it a lot easier for the model to learn than random potentially large values.
                      # It is important to only fit the scaler to the training data, otherwise you are leaking information about the global distribution of variables (which is influenced by the test set) into the training set.

                      scaler = StandardScaler()

                      X_train = scaler.fit_transform(X_train.values)

                      Path("artifact").mkdir(parents=True, exist_ok=True)
                      with open("artifact/test_data.pkl", "wb") as handle:
                          pickle.dump((X_test, y_test), handle)
                      with open("artifact/scaler.pkl", "wb") as handle:
                          pickle.dump(scaler, handle)

                      # Since the dataset is unbalanced (it has many more non-fraud transactions than fraudulent ones), we set a class weight to weight the few fraudulent transactions higher than the many non-fraud transactions.

                      class_weights = class_weight.compute_class_weight('balanced',classes = np.unique(y_train),y = y_train)
                      class_weights = {i : class_weights[i] for i in range(len(class_weights))}

                      # Build the model, the model we build here is a simple fully connected deep neural network, containing 3 hidden layers and one output layer.

                      model = Sequential()
                      model.add(Dense(32, activation = 'relu', input_dim = len(X.columns)))
                      model.add(Dropout(0.2))
                      model.add(Dense(32))
                      model.add(BatchNormalization())
                      model.add(Activation('relu'))
                      model.add(Dropout(0.2))
                      model.add(Dense(32))
                      model.add(BatchNormalization())
                      model.add(Activation('relu'))
                      model.add(Dropout(0.2))
                      model.add(Dense(1, activation = 'sigmoid'))
                      model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])
                      model.summary()

                      # Train the model and get performance

                      epochs = 2
                      history = model.fit(X_train, y_train, epochs=epochs, \
                                          validation_data=(scaler.transform(X_val.values),y_val), \
                                          verbose = True, class_weight = class_weights)

                      # Save the model as ONNX for easy use of ModelMesh

                      model_proto, _ = tf2onnx.convert.from_keras(model)
                      onnx.save(model_proto, output_path)

                  import argparse

                  _parser = argparse.ArgumentParser(prog='Train model',
                  description='')

                  _parser.add_argument("--input", dest="input_path", type=str,
                  required=True, default=argparse.SUPPRESS)

                  _parser.add_argument("--output", dest="output_path",
                  type=_make_parent_dirs_and_return_path, required=True,
                  default=argparse.SUPPRESS)

                  _parsed_args = vars(_parser.parse_args())


                  _outputs = train_model(**_parsed_args)
              computeResources: {}
              env:
                - name: ORIG_PR_NAME
                  valueFrom:
                    fieldRef:
                      fieldPath: 'metadata.labels[''custom.tekton.dev/originalPipelineRun'']'
              image: >-
                quay.io/modh/runtime-images:runtime-cuda-tensorflow-ubi9-python-3.9-2023a-20230817-b7e647e
              name: main
            - command:
                - sh
                - '-ec'
                - >-
                  echo -n "$(context.taskRun.name)" >
                  "$(results.taskrun-name.path)"
              computeResources: {}
              image: busybox
              name: output-taskrun-name
            - command:
                - sh
                - '-ec'
                - >
                  set -exo pipefail

                  TOTAL_SIZE=0

                  copy_artifact() {

                  if [ -d "$1" ]; then
                    tar -czvf "$1".tar.gz "$1"
                    SUFFIX=".tar.gz"
                  fi

                  ARTIFACT_SIZE=`wc -c "$1"${SUFFIX} | awk '{print $1}'`

                  TOTAL_SIZE=$( expr $TOTAL_SIZE + $ARTIFACT_SIZE)

                  touch "$2"

                  if [[ $TOTAL_SIZE -lt 3072 ]]; then
                    if [ -d "$1" ]; then
                      tar -tzf "$1".tar.gz > "$2"
                    elif ! awk "/[^[:print:]]/{f=1} END{exit !f}" "$1"; then
                      cp "$1" "$2"
                    fi
                  fi

                  }

                  copy_artifact
                  $(workspaces.train-model.path)/artifacts/train-upload-stock-kfp-1c509/$(context.taskRun.name)/output
                  $(results.output.path)
              computeResources: {}
              env:
                - name: ORIG_PR_NAME
                  valueFrom:
                    fieldRef:
                      fieldPath: 'metadata.labels[''custom.tekton.dev/originalPipelineRun'']'
              image: busybox
              name: copy-results-artifacts
              onError: continue
            - args:
                - >
                  if [ -d /tekton/results ]; then mkdir -p
                  /tekton/home/tep-results; mv /tekton/results/*
                  /tekton/home/tep-results/ || true; fi
              command:
                - sh
                - '-c'
              computeResources: {}
              image: >-
                registry.access.redhat.com/ubi8/ubi-micro@sha256:396baed3d689157d96aa7d8988fdfea7eb36684c8335eb391cf1952573e689c1
              name: move-all-results-to-tekton-home
            - args:
                - >
                  #!/usr/bin/env sh

                  push_artifact() {
                      workspace_dir=$(echo $(context.taskRun.name) | sed -e "s/$(context.pipeline.name)-//g")
                      workspace_dest=/workspace/${workspace_dir}/artifacts/$(context.pipelineRun.name)/$(context.taskRun.name)
                      artifact_name=$(basename $2)

                      aws_cp() {

                        aws s3 --endpoint https://minio-s3-fraud-detection.apps.aisrhods-wx.8goc.p1.openshiftapps.com cp $1.tgz s3://pipeline-artifacts/artifacts/$PIPELINERUN/$PIPELINETASK/$1.tgz

                      }

                      if [ -f "$workspace_dest/$artifact_name" ]; then
                          echo sending to: ${workspace_dest}/${artifact_name}
                          tar -cvzf $1.tgz -C ${workspace_dest} ${artifact_name}
                          aws_cp $1
                      elif [ -f "$2" ]; then
                          tar -cvzf $1.tgz -C $(dirname $2) ${artifact_name}
                          aws_cp $1
                      else
                          echo "$2 file does not exist. Skip artifact tracking for $1"
                      fi
                  }

                  push_log() {
                      cat /var/log/containers/$PODNAME*$NAMESPACE*step-main*.log > step-main.log
                      push_artifact main-log step-main.log
                  }

                  strip_eof() {
                      if [ -f "$2" ]; then
                          awk 'NF' $2 | head -c -1 > $1_temp_save && cp $1_temp_save $2
                      fi
                  }

                  push_artifact "output" "/tekton/home/tep-results/"$(basename
                  "$(workspaces.train-model.path)/artifacts/train-upload-stock-kfp-1c509/$(context.taskRun.name)/output")
              command:
                - sh
                - '-c'
              computeResources: {}
              env:
                - name: ARTIFACT_BUCKET
                  valueFrom:
                    fieldRef:
                      fieldPath: 'metadata.annotations[''tekton.dev/artifact_bucket'']'
                - name: ARTIFACT_ENDPOINT
                  valueFrom:
                    fieldRef:
                      fieldPath: 'metadata.annotations[''tekton.dev/artifact_endpoint'']'
                - name: ARTIFACT_ENDPOINT_SCHEME
                  valueFrom:
                    fieldRef:
                      fieldPath: >-
                        metadata.annotations['tekton.dev/artifact_endpoint_scheme']
                - name: ARTIFACT_ITEMS
                  valueFrom:
                    fieldRef:
                      fieldPath: 'metadata.annotations[''tekton.dev/artifact_items'']'
                - name: PIPELINETASK
                  valueFrom:
                    fieldRef:
                      fieldPath: 'metadata.labels[''tekton.dev/pipelineTask'']'
                - name: PIPELINERUN
                  valueFrom:
                    fieldRef:
                      fieldPath: 'metadata.labels[''tekton.dev/pipelineRun'']'
                - name: PODNAME
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.name
                - name: NAMESPACE
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.namespace
                - name: AWS_ACCESS_KEY_ID
                  valueFrom:
                    secretKeyRef:
                      key: accesskey
                      name: mlpipeline-minio-artifact
                - name: AWS_SECRET_ACCESS_KEY
                  valueFrom:
                    secretKeyRef:
                      key: secretkey
                      name: mlpipeline-minio-artifact
                - name: ARCHIVE_LOGS
                  value: 'false'
                - name: TRACK_ARTIFACTS
                  value: 'true'
                - name: STRIP_EOF
                  value: 'true'
              image: >-
                registry.redhat.io/rhoai/odh-ml-pipelines-artifact-manager-rhel8@sha256:83248106ac34fb8b9a17fc372f7b6172ef224b79a2627add9e97c7551b2da444
              name: copy-artifacts
          workspaces:
            - name: train-model
        workspaces:
          - name: train-model
            workspace: train-upload-stock-kfp
      - name: upload
        params:
          - name: train-model-trname
            value: $(tasks.train-model.results.taskrun-name)
        runAfter:
          - train-model
        taskSpec:
          metadata:
            annotations:
              pipelines.kubeflow.org/component_spec_digest: >-
                {"name": "Upload", "outputs": [], "version":
                "Upload@sha256=a995d4105a1a6f1ff4ce0660962beddbaa31eb11be8196f0a3682a8fced50a67"}
            labels:
              pipelines.kubeflow.org/cache_enabled: 'true'
          params:
            - name: train-model-trname
              type: string
          spec: null
          steps:
            - args:
                - '--input'
                - >-
                  $(workspaces.upload.path)/artifacts/train-upload-stock-kfp-1c509/$(params.train-model-trname)/output
              command:
                - sh
                - '-c'
                - >-
                  (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
                  --quiet --no-warn-script-location 'boto3' 'botocore' ||
                  PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
                  --no-warn-script-location 'boto3' 'botocore' --user) && "$0"
                  "$@"
                - sh
                - '-ec'
                - |
                  program_path=$(mktemp)
                  printf "%s" "$0" > "$program_path"
                  python3 -u "$program_path" "$@"
                - >
                  def upload(input_path):
                      import os
                      import boto3
                      import botocore

                      aws_access_key_id = os.environ.get('AWS_ACCESS_KEY_ID')
                      aws_secret_access_key = os.environ.get('AWS_SECRET_ACCESS_KEY')
                      endpoint_url = os.environ.get('AWS_S3_ENDPOINT')
                      region_name = os.environ.get('AWS_DEFAULT_REGION')
                      bucket_name = os.environ.get('AWS_S3_BUCKET')

                      s3_key = os.environ.get("S3_KEY")

                      session = boto3.session.Session(aws_access_key_id=aws_access_key_id,
                                                      aws_secret_access_key=aws_secret_access_key)

                      s3_resource = session.resource(
                          's3',
                          config=botocore.client.Config(signature_version='s3v4'),
                          endpoint_url=endpoint_url,
                          region_name=region_name)

                      bucket = s3_resource.Bucket(bucket_name)

                      print(f"Uploading {s3_key}")
                      bucket.upload_file(input_path, s3_key)

                  import argparse

                  _parser = argparse.ArgumentParser(prog='Upload',
                  description='')

                  _parser.add_argument("--input", dest="input_path", type=str,
                  required=True, default=argparse.SUPPRESS)

                  _parsed_args = vars(_parser.parse_args())


                  _outputs = upload(**_parsed_args)
              computeResources: {}
              env:
                - name: S3_KEY
                  value: models/fraud/1/model.onnx
                - name: ORIG_PR_NAME
                  valueFrom:
                    fieldRef:
                      fieldPath: 'metadata.labels[''custom.tekton.dev/originalPipelineRun'']'
              envFrom:
                - secretRef:
                    name: aws-connection-my-storage
              image: >-
                quay.io/modh/runtime-images:runtime-cuda-tensorflow-ubi9-python-3.9-2023a-20230817-b7e647e
              name: main
          workspaces:
            - name: upload
        workspaces:
          - name: upload
            workspace: train-upload-stock-kfp
    workspaces:
      - name: train-upload-stock-kfp
  taskRunTemplate:
    serviceAccountName: pipeline-runner-pipelines-definition
  timeouts:
    pipeline: 1h0m0s
  workspaces:
    - name: train-upload-stock-kfp
      volumeClaimTemplate:
        metadata:
          creationTimestamp: null
        spec:
          accessModes:
            - ReadWriteOnce
          resources:
            requests:
              storage: 2Gi
        status: {}
status:
  childReferences:
    - apiVersion: tekton.dev/v1
      kind: TaskRun
      name: train-upload-stock-kfp-1c509-get-data
      pipelineTaskName: get-data
    - apiVersion: tekton.dev/v1
      kind: TaskRun
      name: train-upload-stock-kfp-1c509-train-model
      pipelineTaskName: train-model
  conditions:
    - lastTransitionTime: '2024-02-23T14:08:50Z'
      message: 'Tasks Completed: 1 (Failed: 0, Cancelled 0), Incomplete: 2, Skipped: 0'
      reason: Running
      status: Unknown
      type: Succeeded
  pipelineSpec:
    tasks:
      - name: get-data
        taskSpec:
          metadata:
            annotations:
              pipelines.kubeflow.org/component_spec_digest: >-
                {"name": "Get data", "outputs": [{"name": "output"}], "version":
                "Get
                data@sha256=a840b31ffc61f2af6a40651f5ee87a07d2c5945073e00ebc9470ccdd8c5bfdfd"}
            labels:
              pipelines.kubeflow.org/cache_enabled: 'true'
          results:
            - description: /tmp/outputs/output/data
              name: output
              type: string
            - name: taskrun-name
              type: string
          spec: null
          steps:
            - args:
                - '--output'
                - >-
                  $(workspaces.get-data.path)/artifacts/train-upload-stock-kfp-1c509/$(context.taskRun.name)/output
              command:
                - sh
                - '-ec'
                - |
                  program_path=$(mktemp)
                  printf "%s" "$0" > "$program_path"
                  python3 -u "$program_path" "$@"
                - >
                  def _make_parent_dirs_and_return_path(file_path: str):
                      import os
                      os.makedirs(os.path.dirname(file_path), exist_ok=True)
                      return file_path

                  def get_data(output_path):
                      import urllib.request
                      print("starting download...")
                      url = "https://github.com/cfchase/fraud-detection-notebooks/raw/main/data/card_transdata.csv"
                      urllib.request.urlretrieve(url, output_path)
                      print("done")

                  import argparse

                  _parser = argparse.ArgumentParser(prog='Get data',
                  description='')

                  _parser.add_argument("--output", dest="output_path",
                  type=_make_parent_dirs_and_return_path, required=True,
                  default=argparse.SUPPRESS)

                  _parsed_args = vars(_parser.parse_args())


                  _outputs = get_data(**_parsed_args)
              computeResources: {}
              env:
                - name: ORIG_PR_NAME
                  valueFrom:
                    fieldRef:
                      fieldPath: 'metadata.labels[''custom.tekton.dev/originalPipelineRun'']'
              image: >-
                quay.io/modh/runtime-images:runtime-cuda-tensorflow-ubi9-python-3.9-2023a-20230817-b7e647e
              name: main
            - command:
                - sh
                - '-ec'
                - >-
                  echo -n "$(context.taskRun.name)" >
                  "$(results.taskrun-name.path)"
              computeResources: {}
              image: busybox
              name: output-taskrun-name
            - command:
                - sh
                - '-ec'
                - >
                  set -exo pipefail

                  TOTAL_SIZE=0

                  copy_artifact() {

                  if [ -d "$1" ]; then
                    tar -czvf "$1".tar.gz "$1"
                    SUFFIX=".tar.gz"
                  fi

                  ARTIFACT_SIZE=`wc -c "$1"${SUFFIX} | awk '{print $1}'`

                  TOTAL_SIZE=$( expr $TOTAL_SIZE + $ARTIFACT_SIZE)

                  touch "$2"

                  if [[ $TOTAL_SIZE -lt 3072 ]]; then
                    if [ -d "$1" ]; then
                      tar -tzf "$1".tar.gz > "$2"
                    elif ! awk "/[^[:print:]]/{f=1} END{exit !f}" "$1"; then
                      cp "$1" "$2"
                    fi
                  fi

                  }

                  copy_artifact
                  $(workspaces.get-data.path)/artifacts/train-upload-stock-kfp-1c509/$(context.taskRun.name)/output
                  $(results.output.path)
              computeResources: {}
              env:
                - name: ORIG_PR_NAME
                  valueFrom:
                    fieldRef:
                      fieldPath: 'metadata.labels[''custom.tekton.dev/originalPipelineRun'']'
              image: busybox
              name: copy-results-artifacts
              onError: continue
            - args:
                - >
                  if [ -d /tekton/results ]; then mkdir -p
                  /tekton/home/tep-results; mv /tekton/results/*
                  /tekton/home/tep-results/ || true; fi
              command:
                - sh
                - '-c'
              computeResources: {}
              image: >-
                registry.access.redhat.com/ubi8/ubi-micro@sha256:396baed3d689157d96aa7d8988fdfea7eb36684c8335eb391cf1952573e689c1
              name: move-all-results-to-tekton-home
            - args:
                - >
                  #!/usr/bin/env sh

                  push_artifact() {
                      workspace_dir=$(echo $(context.taskRun.name) | sed -e "s/train-upload-stock-kfp-1c509-//g")
                      workspace_dest=/workspace/${workspace_dir}/artifacts/train-upload-stock-kfp-1c509/$(context.taskRun.name)
                      artifact_name=$(basename $2)

                      aws_cp() {

                        aws s3 --endpoint https://minio-s3-fraud-detection.apps.aisrhods-wx.8goc.p1.openshiftapps.com cp $1.tgz s3://pipeline-artifacts/artifacts/$PIPELINERUN/$PIPELINETASK/$1.tgz

                      }

                      if [ -f "$workspace_dest/$artifact_name" ]; then
                          echo sending to: ${workspace_dest}/${artifact_name}
                          tar -cvzf $1.tgz -C ${workspace_dest} ${artifact_name}
                          aws_cp $1
                      elif [ -f "$2" ]; then
                          tar -cvzf $1.tgz -C $(dirname $2) ${artifact_name}
                          aws_cp $1
                      else
                          echo "$2 file does not exist. Skip artifact tracking for $1"
                      fi
                  }

                  push_log() {
                      cat /var/log/containers/$PODNAME*$NAMESPACE*step-main*.log > step-main.log
                      push_artifact main-log step-main.log
                  }

                  strip_eof() {
                      if [ -f "$2" ]; then
                          awk 'NF' $2 | head -c -1 > $1_temp_save && cp $1_temp_save $2
                      fi
                  }

                  push_artifact "output" "/tekton/home/tep-results/"$(basename
                  "$(workspaces.get-data.path)/artifacts/train-upload-stock-kfp-1c509/$(context.taskRun.name)/output")
              command:
                - sh
                - '-c'
              computeResources: {}
              env:
                - name: ARTIFACT_BUCKET
                  valueFrom:
                    fieldRef:
                      fieldPath: 'metadata.annotations[''tekton.dev/artifact_bucket'']'
                - name: ARTIFACT_ENDPOINT
                  valueFrom:
                    fieldRef:
                      fieldPath: 'metadata.annotations[''tekton.dev/artifact_endpoint'']'
                - name: ARTIFACT_ENDPOINT_SCHEME
                  valueFrom:
                    fieldRef:
                      fieldPath: >-
                        metadata.annotations['tekton.dev/artifact_endpoint_scheme']
                - name: ARTIFACT_ITEMS
                  valueFrom:
                    fieldRef:
                      fieldPath: 'metadata.annotations[''tekton.dev/artifact_items'']'
                - name: PIPELINETASK
                  valueFrom:
                    fieldRef:
                      fieldPath: 'metadata.labels[''tekton.dev/pipelineTask'']'
                - name: PIPELINERUN
                  valueFrom:
                    fieldRef:
                      fieldPath: 'metadata.labels[''tekton.dev/pipelineRun'']'
                - name: PODNAME
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.name
                - name: NAMESPACE
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.namespace
                - name: AWS_ACCESS_KEY_ID
                  valueFrom:
                    secretKeyRef:
                      key: accesskey
                      name: mlpipeline-minio-artifact
                - name: AWS_SECRET_ACCESS_KEY
                  valueFrom:
                    secretKeyRef:
                      key: secretkey
                      name: mlpipeline-minio-artifact
                - name: ARCHIVE_LOGS
                  value: 'false'
                - name: TRACK_ARTIFACTS
                  value: 'true'
                - name: STRIP_EOF
                  value: 'true'
              image: >-
                registry.redhat.io/rhoai/odh-ml-pipelines-artifact-manager-rhel8@sha256:83248106ac34fb8b9a17fc372f7b6172ef224b79a2627add9e97c7551b2da444
              name: copy-artifacts
          workspaces:
            - name: get-data
        workspaces:
          - name: get-data
            workspace: train-upload-stock-kfp
      - name: train-model
        params:
          - name: get-data-trname
            value: $(tasks.get-data.results.taskrun-name)
        runAfter:
          - get-data
        taskSpec:
          metadata:
            annotations:
              pipelines.kubeflow.org/component_spec_digest: >-
                {"name": "Train model", "outputs": [{"name": "output"}],
                "version": "Train
                model@sha256=eaddeda6a61a76fc58aef31f1b7e762a9f8354045a1cde36986495e6568341ba"}
            labels:
              pipelines.kubeflow.org/cache_enabled: 'true'
          params:
            - name: get-data-trname
              type: string
          results:
            - description: /tmp/outputs/output/data
              name: output
              type: string
            - name: taskrun-name
              type: string
          spec: null
          steps:
            - args:
                - '--input'
                - >-
                  $(workspaces.train-model.path)/artifacts/train-upload-stock-kfp-1c509/$(params.get-data-trname)/output
                - '--output'
                - >-
                  $(workspaces.train-model.path)/artifacts/train-upload-stock-kfp-1c509/$(context.taskRun.name)/output
              command:
                - sh
                - '-c'
                - >-
                  (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
                  --quiet --no-warn-script-location 'tf2onnx' 'seaborn' ||
                  PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
                  --no-warn-script-location 'tf2onnx' 'seaborn' --user) && "$0"
                  "$@"
                - sh
                - '-ec'
                - |
                  program_path=$(mktemp)
                  printf "%s" "$0" > "$program_path"
                  python3 -u "$program_path" "$@"
                - >
                  def _make_parent_dirs_and_return_path(file_path: str):
                      import os
                      os.makedirs(os.path.dirname(file_path), exist_ok=True)
                      return file_path

                  def train_model(input_path, output_path):
                      import numpy as np
                      import pandas as pd
                      from keras.models import Sequential
                      from keras.layers import Dense, Dropout, BatchNormalization, Activation
                      from sklearn.model_selection import train_test_split
                      from sklearn.preprocessing import StandardScaler
                      from sklearn.utils import class_weight
                      import tf2onnx
                      import onnx
                      import pickle
                      from pathlib import Path

                      # Load the CSV data which we will use to train the model.
                      # It contains the following fields:
                      #   distancefromhome - The distance from home where the transaction happened.
                      #   distancefromlast_transaction - The distance from last transaction happened.
                      #   ratiotomedianpurchaseprice - Ratio of purchased price compared to median purchase price.
                      #   repeat_retailer - If it's from a retailer that already has been purchased from before.
                      #   used_chip - If the (credit card) chip was used.
                      #   usedpinnumber - If the PIN number was used.
                      #   online_order - If it was an online order.
                      #   fraud - If the transaction is fraudulent.
                      Data = pd.read_csv(input_path)

                      # Set the input (X) and output (Y) data.
                      # The only output data we have is if it's fraudulent or not, and all other fields go as inputs to the model.

                      X = Data.drop(columns = ['repeat_retailer','distance_from_home', 'fraud'])
                      y = Data['fraud']

                      # Split the data into training and testing sets so we have something to test the trained model with.

                      # X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, stratify = y)
                      X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, shuffle = False)

                      X_train, X_val, y_train, y_val = train_test_split(X_train,y_train, test_size = 0.2, stratify = y_train)

                      # Scale the data to remove mean and have unit variance. This means that the data will be between -1 and 1, which makes it a lot easier for the model to learn than random potentially large values.
                      # It is important to only fit the scaler to the training data, otherwise you are leaking information about the global distribution of variables (which is influenced by the test set) into the training set.

                      scaler = StandardScaler()

                      X_train = scaler.fit_transform(X_train.values)

                      Path("artifact").mkdir(parents=True, exist_ok=True)
                      with open("artifact/test_data.pkl", "wb") as handle:
                          pickle.dump((X_test, y_test), handle)
                      with open("artifact/scaler.pkl", "wb") as handle:
                          pickle.dump(scaler, handle)

                      # Since the dataset is unbalanced (it has many more non-fraud transactions than fraudulent ones), we set a class weight to weight the few fraudulent transactions higher than the many non-fraud transactions.

                      class_weights = class_weight.compute_class_weight('balanced',classes = np.unique(y_train),y = y_train)
                      class_weights = {i : class_weights[i] for i in range(len(class_weights))}

                      # Build the model, the model we build here is a simple fully connected deep neural network, containing 3 hidden layers and one output layer.

                      model = Sequential()
                      model.add(Dense(32, activation = 'relu', input_dim = len(X.columns)))
                      model.add(Dropout(0.2))
                      model.add(Dense(32))
                      model.add(BatchNormalization())
                      model.add(Activation('relu'))
                      model.add(Dropout(0.2))
                      model.add(Dense(32))
                      model.add(BatchNormalization())
                      model.add(Activation('relu'))
                      model.add(Dropout(0.2))
                      model.add(Dense(1, activation = 'sigmoid'))
                      model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])
                      model.summary()

                      # Train the model and get performance

                      epochs = 2
                      history = model.fit(X_train, y_train, epochs=epochs, \
                                          validation_data=(scaler.transform(X_val.values),y_val), \
                                          verbose = True, class_weight = class_weights)

                      # Save the model as ONNX for easy use of ModelMesh

                      model_proto, _ = tf2onnx.convert.from_keras(model)
                      onnx.save(model_proto, output_path)

                  import argparse

                  _parser = argparse.ArgumentParser(prog='Train model',
                  description='')

                  _parser.add_argument("--input", dest="input_path", type=str,
                  required=True, default=argparse.SUPPRESS)

                  _parser.add_argument("--output", dest="output_path",
                  type=_make_parent_dirs_and_return_path, required=True,
                  default=argparse.SUPPRESS)

                  _parsed_args = vars(_parser.parse_args())


                  _outputs = train_model(**_parsed_args)
              computeResources: {}
              env:
                - name: ORIG_PR_NAME
                  valueFrom:
                    fieldRef:
                      fieldPath: 'metadata.labels[''custom.tekton.dev/originalPipelineRun'']'
              image: >-
                quay.io/modh/runtime-images:runtime-cuda-tensorflow-ubi9-python-3.9-2023a-20230817-b7e647e
              name: main
            - command:
                - sh
                - '-ec'
                - >-
                  echo -n "$(context.taskRun.name)" >
                  "$(results.taskrun-name.path)"
              computeResources: {}
              image: busybox
              name: output-taskrun-name
            - command:
                - sh
                - '-ec'
                - >
                  set -exo pipefail

                  TOTAL_SIZE=0

                  copy_artifact() {

                  if [ -d "$1" ]; then
                    tar -czvf "$1".tar.gz "$1"
                    SUFFIX=".tar.gz"
                  fi

                  ARTIFACT_SIZE=`wc -c "$1"${SUFFIX} | awk '{print $1}'`

                  TOTAL_SIZE=$( expr $TOTAL_SIZE + $ARTIFACT_SIZE)

                  touch "$2"

                  if [[ $TOTAL_SIZE -lt 3072 ]]; then
                    if [ -d "$1" ]; then
                      tar -tzf "$1".tar.gz > "$2"
                    elif ! awk "/[^[:print:]]/{f=1} END{exit !f}" "$1"; then
                      cp "$1" "$2"
                    fi
                  fi

                  }

                  copy_artifact
                  $(workspaces.train-model.path)/artifacts/train-upload-stock-kfp-1c509/$(context.taskRun.name)/output
                  $(results.output.path)
              computeResources: {}
              env:
                - name: ORIG_PR_NAME
                  valueFrom:
                    fieldRef:
                      fieldPath: 'metadata.labels[''custom.tekton.dev/originalPipelineRun'']'
              image: busybox
              name: copy-results-artifacts
              onError: continue
            - args:
                - >
                  if [ -d /tekton/results ]; then mkdir -p
                  /tekton/home/tep-results; mv /tekton/results/*
                  /tekton/home/tep-results/ || true; fi
              command:
                - sh
                - '-c'
              computeResources: {}
              image: >-
                registry.access.redhat.com/ubi8/ubi-micro@sha256:396baed3d689157d96aa7d8988fdfea7eb36684c8335eb391cf1952573e689c1
              name: move-all-results-to-tekton-home
            - args:
                - >
                  #!/usr/bin/env sh

                  push_artifact() {
                      workspace_dir=$(echo $(context.taskRun.name) | sed -e "s/train-upload-stock-kfp-1c509-//g")
                      workspace_dest=/workspace/${workspace_dir}/artifacts/train-upload-stock-kfp-1c509/$(context.taskRun.name)
                      artifact_name=$(basename $2)

                      aws_cp() {

                        aws s3 --endpoint https://minio-s3-fraud-detection.apps.aisrhods-wx.8goc.p1.openshiftapps.com cp $1.tgz s3://pipeline-artifacts/artifacts/$PIPELINERUN/$PIPELINETASK/$1.tgz

                      }

                      if [ -f "$workspace_dest/$artifact_name" ]; then
                          echo sending to: ${workspace_dest}/${artifact_name}
                          tar -cvzf $1.tgz -C ${workspace_dest} ${artifact_name}
                          aws_cp $1
                      elif [ -f "$2" ]; then
                          tar -cvzf $1.tgz -C $(dirname $2) ${artifact_name}
                          aws_cp $1
                      else
                          echo "$2 file does not exist. Skip artifact tracking for $1"
                      fi
                  }

                  push_log() {
                      cat /var/log/containers/$PODNAME*$NAMESPACE*step-main*.log > step-main.log
                      push_artifact main-log step-main.log
                  }

                  strip_eof() {
                      if [ -f "$2" ]; then
                          awk 'NF' $2 | head -c -1 > $1_temp_save && cp $1_temp_save $2
                      fi
                  }

                  push_artifact "output" "/tekton/home/tep-results/"$(basename
                  "$(workspaces.train-model.path)/artifacts/train-upload-stock-kfp-1c509/$(context.taskRun.name)/output")
              command:
                - sh
                - '-c'
              computeResources: {}
              env:
                - name: ARTIFACT_BUCKET
                  valueFrom:
                    fieldRef:
                      fieldPath: 'metadata.annotations[''tekton.dev/artifact_bucket'']'
                - name: ARTIFACT_ENDPOINT
                  valueFrom:
                    fieldRef:
                      fieldPath: 'metadata.annotations[''tekton.dev/artifact_endpoint'']'
                - name: ARTIFACT_ENDPOINT_SCHEME
                  valueFrom:
                    fieldRef:
                      fieldPath: >-
                        metadata.annotations['tekton.dev/artifact_endpoint_scheme']
                - name: ARTIFACT_ITEMS
                  valueFrom:
                    fieldRef:
                      fieldPath: 'metadata.annotations[''tekton.dev/artifact_items'']'
                - name: PIPELINETASK
                  valueFrom:
                    fieldRef:
                      fieldPath: 'metadata.labels[''tekton.dev/pipelineTask'']'
                - name: PIPELINERUN
                  valueFrom:
                    fieldRef:
                      fieldPath: 'metadata.labels[''tekton.dev/pipelineRun'']'
                - name: PODNAME
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.name
                - name: NAMESPACE
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.namespace
                - name: AWS_ACCESS_KEY_ID
                  valueFrom:
                    secretKeyRef:
                      key: accesskey
                      name: mlpipeline-minio-artifact
                - name: AWS_SECRET_ACCESS_KEY
                  valueFrom:
                    secretKeyRef:
                      key: secretkey
                      name: mlpipeline-minio-artifact
                - name: ARCHIVE_LOGS
                  value: 'false'
                - name: TRACK_ARTIFACTS
                  value: 'true'
                - name: STRIP_EOF
                  value: 'true'
              image: >-
                registry.redhat.io/rhoai/odh-ml-pipelines-artifact-manager-rhel8@sha256:83248106ac34fb8b9a17fc372f7b6172ef224b79a2627add9e97c7551b2da444
              name: copy-artifacts
          workspaces:
            - name: train-model
        workspaces:
          - name: train-model
            workspace: train-upload-stock-kfp
      - name: upload
        params:
          - name: train-model-trname
            value: $(tasks.train-model.results.taskrun-name)
        runAfter:
          - train-model
        taskSpec:
          metadata:
            annotations:
              pipelines.kubeflow.org/component_spec_digest: >-
                {"name": "Upload", "outputs": [], "version":
                "Upload@sha256=a995d4105a1a6f1ff4ce0660962beddbaa31eb11be8196f0a3682a8fced50a67"}
            labels:
              pipelines.kubeflow.org/cache_enabled: 'true'
          params:
            - name: train-model-trname
              type: string
          spec: null
          steps:
            - args:
                - '--input'
                - >-
                  $(workspaces.upload.path)/artifacts/train-upload-stock-kfp-1c509/$(params.train-model-trname)/output
              command:
                - sh
                - '-c'
                - >-
                  (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
                  --quiet --no-warn-script-location 'boto3' 'botocore' ||
                  PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
                  --no-warn-script-location 'boto3' 'botocore' --user) && "$0"
                  "$@"
                - sh
                - '-ec'
                - |
                  program_path=$(mktemp)
                  printf "%s" "$0" > "$program_path"
                  python3 -u "$program_path" "$@"
                - >
                  def upload(input_path):
                      import os
                      import boto3
                      import botocore

                      aws_access_key_id = os.environ.get('AWS_ACCESS_KEY_ID')
                      aws_secret_access_key = os.environ.get('AWS_SECRET_ACCESS_KEY')
                      endpoint_url = os.environ.get('AWS_S3_ENDPOINT')
                      region_name = os.environ.get('AWS_DEFAULT_REGION')
                      bucket_name = os.environ.get('AWS_S3_BUCKET')

                      s3_key = os.environ.get("S3_KEY")

                      session = boto3.session.Session(aws_access_key_id=aws_access_key_id,
                                                      aws_secret_access_key=aws_secret_access_key)

                      s3_resource = session.resource(
                          's3',
                          config=botocore.client.Config(signature_version='s3v4'),
                          endpoint_url=endpoint_url,
                          region_name=region_name)

                      bucket = s3_resource.Bucket(bucket_name)

                      print(f"Uploading {s3_key}")
                      bucket.upload_file(input_path, s3_key)

                  import argparse

                  _parser = argparse.ArgumentParser(prog='Upload',
                  description='')

                  _parser.add_argument("--input", dest="input_path", type=str,
                  required=True, default=argparse.SUPPRESS)

                  _parsed_args = vars(_parser.parse_args())


                  _outputs = upload(**_parsed_args)
              computeResources: {}
              env:
                - name: S3_KEY
                  value: models/fraud/1/model.onnx
                - name: ORIG_PR_NAME
                  valueFrom:
                    fieldRef:
                      fieldPath: 'metadata.labels[''custom.tekton.dev/originalPipelineRun'']'
              envFrom:
                - secretRef:
                    name: aws-connection-my-storage
              image: >-
                quay.io/modh/runtime-images:runtime-cuda-tensorflow-ubi9-python-3.9-2023a-20230817-b7e647e
              name: main
          workspaces:
            - name: upload
        workspaces:
          - name: upload
            workspace: train-upload-stock-kfp
    workspaces:
      - name: train-upload-stock-kfp
  startTime: '2024-02-23T14:08:23Z'
